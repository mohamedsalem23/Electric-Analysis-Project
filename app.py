# ====== 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø³Ø±ÙŠØ¹Ø© ======
import os, streamlit as st, pandas as pd, re, io, base64
from typing import List
from PIL import Image
from sentence_transformers import SentenceTransformer
import numpy as np

# âœ… Ø¥Ø¶Ø§ÙØ§Øª PDF
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak, KeepTogether
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.colors import HexColor, black, grey
from datetime import datetime

# âœ… Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„
from reportlab.lib.enums import TA_RIGHT, TA_CENTER
from bidi import algorithm as bidi_algorithm
from arabic_reshaper import reshape 

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
# âœ… Ø¥Ø¬Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… REST Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† gRPC
os.environ["GOOGLE_API_USE_CLIENT_CERTIFICATE"] = "false"
os.environ["GRPC_ENABLE_FORK_SUPPORT"] = "false"

# âœ… Ù‚Ø±Ø§Ø¡Ø© API Key Ù…Ù† Streamlit Secrets
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    if not GEMINI_API_KEY:
        st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© GEMINI_API_KEY ÙÙŠ Settings â†’ Secrets Ø¹Ù„Ù‰ Streamlit Cloud")
        st.info("Ù„Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø­Ù„ÙŠ: Ø£Ù†Ø´Ø¦ Ù…Ù„Ù .streamlit/secrets.toml ÙˆØ¶Ø¹ ÙÙŠÙ‡: GEMINI_API_KEY = 'your_key'")
        st.stop()

# ====== 2. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ======
def pil_to_base64_uri(image: Image.Image, fmt="PNG") -> str:
    buf = io.BytesIO()
    image.save(buf, format=fmt)
    img_bytes = buf.getvalue()
    return f"data:image/{fmt.lower()};base64,{base64.b64encode(img_bytes).decode()}"

@st.cache_data(show_spinner=False)
def load_excel() -> pd.DataFrame:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Excel Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ"""
    try:
        # âœ… Ù…Ø³Ø§Ø± Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù€ Deploy
        excel_path = os.path.join(os.path.dirname(__file__), "data", "Copy of Ø¬Ù…ÙŠØ¹_Ø¨Ù†ÙˆØ¯_ÙØ­Øµ_Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡(1).xlsx")
        
        if not os.path.exists(excel_path):
            st.error(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {excel_path}")
            st.info("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ data/")
            # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            return pd.DataFrame({
                "Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯": [5]*4,
                "Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯": ["Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©"]*4,
                "Ø§Ù„Ù…ØªØ·Ù„Ø¨": ["..."],
                "Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ": ["..."],
                "Ø§Ù„ØªÙˆØµÙŠØ§Øª": ["ÙŠØ¬Ø¨ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙØªØ§Ø­ Ø¬ÙŠØ¯Ø§Ù‹.; ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø­ÙˆÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±."],
                "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­": ["Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠÙ„ÙŠÙƒÙˆÙ† Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§ØºØ§Øª.; Ø¥Ø¹Ø§Ø¯Ø© ØªØ«Ø¨ÙŠØª Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚ÙŠÙ…."],
                "Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)": [35,30,40,25]
            })
        
        return pd.read_excel(excel_path)
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
        return pd.DataFrame()

@st.cache_data(show_spinner=False)
def df_to_docs(df: pd.DataFrame) -> List[dict]:
    """ØªØ­ÙˆÙŠÙ„ DataFrame Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
    return [
        {
            'content': f"Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯: {r['Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯']}. Ø§Ù„Ù…ØªØ·Ù„Ø¨: {r['Ø§Ù„Ù…ØªØ·Ù„Ø¨']}.",
            'metadata': r.to_dict()
        }
        for _, r in df.iterrows()
    ]

def filter_best_doc(similar_docs: List[dict], query: str) -> int:
    """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…Ø³ØªÙ†Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡"""
    best_doc = None
    best_score = 0.0
    for doc in similar_docs:
        name = doc['metadata'].get('Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯', '')
        match_score = len(set(re.findall(r'\w+', query.lower())) & set(re.findall(r'\w+', name.lower()))) / max(len(set(re.findall(r'\w+', query.lower()))), 1)
        if match_score > best_score:
            best_score = match_score
            best_doc = doc
    return int(best_doc['metadata'].get('Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 0)) if best_doc else int(similar_docs[0]['metadata'].get('Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 0))

def build_table_from_band(dataframe: pd.DataFrame, band_num: int, query: str) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ù…Ù† Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯ Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ ÙŠØ¯ÙˆÙŠ Ù„Ù€ Markdown"""
    band_rows = dataframe[dataframe['Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯'] == band_num].copy()
    if band_rows.empty:
        return "| Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª |"
    
    def match_score(row):
        req = str(row.get('Ø§Ù„Ù…ØªØ·Ù„Ø¨', '')).lower()
        q_words = set(re.findall(r'\w+', query.lower()))
        row_words = set(re.findall(r'\w+', req))
        return len(q_words & row_words) / max(len(q_words), 1)
    
    band_rows['match_score'] = band_rows.apply(match_score, axis=1)
    best_row = band_rows.loc[band_rows['match_score'].idxmax()]
    
    cols = ['Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 'Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯', 'Ø§Ù„Ù…ØªØ·Ù„Ø¨', 'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ', 'Ø§Ù„ØªÙˆØµÙŠØ§Øª', 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­', 'Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)']
    best_row = best_row[cols].to_frame().T
    
    # âœ… ØªØ­ÙˆÙŠÙ„ ÙŠØ¯ÙˆÙŠ Ø¥Ù„Ù‰ Markdown Ø¨Ø¯ÙˆÙ† tabulate
    markdown_lines = []
    
    # Header
    header = "| " + " | ".join(cols) + " |"
    markdown_lines.append(header)
    
    # Separator
    separator = "|" + "|".join([" --- " for _ in cols]) + "|"
    markdown_lines.append(separator)
    
    # Data row
    row_data = []
    for col in cols:
        value = str(best_row[col].values[0]) if col in best_row.columns else "â€”"
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚ÙŠÙ…Ø©
        value = value.replace('\n', ' ').replace('|', '\\|')
        row_data.append(value)
    
    data_row = "| " + " | ".join(row_data) + " |"
    markdown_lines.append(data_row)
    
    return "\n".join(markdown_lines)

@st.cache_resource(show_spinner=False)
def get_models():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings"""
    try:
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings")
        return model
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
        return None

embedding_model = get_models()

@st.cache_resource(show_spinner=False)
def get_vector_db(_docs: List[dict], _model):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª embeddings Ø¨Ø³ÙŠØ·Ø©"""
    try:
        if _model is None:
            return None, None
            
        # âœ… Ø­Ø³Ø§Ø¨ embeddings Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        texts = [doc['content'] for doc in _docs]
        embeddings = _model.encode(texts, show_progress_bar=False)
        
        st.success("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return embeddings, texts
    except Exception as e:
        st.warning(f"âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¨Ø³ÙŠØ·: {e}")
        return None, None

def cosine_similarity(a, b):
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…ØªØ¬Ù‡ÙŠÙ†"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def search_similar(query: str, docs: List[dict], embeddings_matrix, texts, model, k: int = 3) -> List[dict]:
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
    try:
        if embeddings_matrix is None or model is None:
            return simple_search(docs, query, k)
        
        # ØªØ´ÙÙŠØ± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_embedding = model.encode([query], show_progress_bar=False)[0]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarities = [cosine_similarity(query_embedding, emb) for emb in embeddings_matrix]
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ k Ù†ØªÙŠØ¬Ø©
        top_indices = np.argsort(similarities)[-k:][::-1]
        
        return [docs[i] for i in top_indices]
    except:
        return simple_search(docs, query, k)

def simple_search(docs: List[dict], query: str, k: int = 3) -> List[dict]:
    """Ø¨Ø­Ø« Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† embeddings"""
    query_words = set(query.lower().split())
    
    scores = []
    for doc in docs:
        doc_words = set(doc['content'].lower().split())
        intersection = len(query_words & doc_words)
        union = len(query_words | doc_words)
        score = intersection / union if union > 0 else 0
        scores.append((doc, score))
    
    scores.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scores[:k]]

@st.cache_data(show_spinner=False)
def batch_analyze(images_bytes: List[bytes]) -> List[str]:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini Vision Ù…Ø¹ REST API"""
    try:
        import google.generativeai as genai
        import requests
        
        # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… REST API Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† SDK
        API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
        
        prompt_text = """
Ø£Ù†Øª Ù†Ø¸Ø§Ù… Ø±Ø¤ÙŠØ© Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØªØ­Ø¯ÙŠØ¯ **Ø¬Ù…ÙŠØ¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©** Ø§Ù„Ù„ÙŠ ØªØ¸Ù‡Ø± (Ø­ØªÙ‰ Ù„Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† ÙˆØ§Ø­Ø¯Ø©ØŒ Ù…Ø«Ù„ ÙØ±Ø§ØºØ§Øª + Ù…ÙŠÙ„Ø§Ù† + Ø¨Ø±ÙˆØ²). 
**Ù„ÙƒÙ„ Ø¹ÙŠØ¨ØŒ Ø£Ø¹Ø·Ù Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ (Ø£Ùˆ Ø§Ù„Ø£Ù‚Ø±Ø¨) Ù…Ù† Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©**ØŒ ÙˆÙØµÙ„Ù‡Ø§ Ø¨Ù€ ';' (Ù…Ø«Ù„: 'Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©; Ø§Ø³ØªÙ‚Ø§Ù…Ø© Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø£ÙÙ‚ÙŠÙ‹Ø§').
Ù„Ùˆ Ø¹ÙŠØ¨ ÙˆØ§Ø­Ø¯ØŒ Ø£Ø¹Ø·Ù Ø§Ø³Ù…Ù‡ Ø¨Ø³. Ù„Ø§ ØªØ¶Ù ØªÙØ³ÙŠØ± Ø£Ùˆ Ø´Ø±Ø­ØŒ Ù†Ø§ØªØ¬Ùƒ Ù†Øµ ÙˆØ§Ø­Ø¯ Ù…ÙØµÙˆÙ„ Ø¨Ù€ ';'.
"""
        
        defects = []
        
        for img_bytes in images_bytes:
            img = Image.open(io.BytesIO(img_bytes))
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB Ù„Ø­Ù„ Ø®Ø·Ø£ "cannot write mode RGBA as JPEG"
            if img.mode in ("RGBA", "LA") or (img.mode == "P" and 'transparency' in img.info):
                img = img.convert("RGB")
            else:
                # Ø¶Ù…Ù†Ø§Ù‹ Ù†Ø­ÙˆÙ„ Ø£ÙŠ ÙˆØ¶Ø¹ ØºÙŠØ± RGB Ø¥Ù„Ù‰ RGB Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸ ÙƒÙ€ JPEG
                if img.mode != "RGB":
                    img = img.convert("RGB")
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ base64
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ù„Ø¨
            payload = {
                "contents": [{
                    "parts": [
                        {"text": prompt_text},
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": img_base64
                            }
                        }
                    ]
                }]
            }
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨
            response = requests.post(API_URL, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    text = result['candidates'][0]['content']['parts'][0]['text']
                    lines = text.strip().splitlines()
                    for line in lines:
                        defects.extend([x.strip() for x in line.split(";") if x.strip()])
        
        return defects if defects else ["Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©"]
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±: {e}")
        # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        return ["Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©", "Ø§Ø³ØªÙ‚Ø§Ù…Ø© Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø£ÙÙ‚ÙŠØ§"]

def clean_text_for_pdf(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† HTML tags ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©"""
    if not text or text == "nan" or pd.isna(text):
        return "â€”"
    
    text = str(text).strip()
    
    # âœ… Ø¥Ø²Ø§Ù„Ø© HTML tags
    text = re.sub(r'<br\s*/?>', '\n', text)  # ØªØ­ÙˆÙŠÙ„ <br/> Ù„Ù€ newline
    text = re.sub(r'<[^>]+>', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ tags Ø£Ø®Ø±Ù‰
    
    # âœ… ØªÙ†Ø¸ÙŠÙ Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&amp;', '&')
    
    # âœ… ØªÙ†Ø¸ÙŠÙ Ù…Ø³Ø§ÙØ§Øª Ø²Ø§Ø¦Ø¯Ø©
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()

# âœ… Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def process_arabic_text(text: str) -> str:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ø¹ Ø¯Ø¹Ù… RTL"""
    # ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„Ø§Ù‹
    text = clean_text_for_pdf(text)
    
    if not text or text == "â€”":
        return "â€”"
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    reshaped = reshape(text)
    # ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© BiDi
    bidi_text = bidi_algorithm.get_display(reshaped)
    return bidi_text

# âœ… Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Markdown Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def clean_markdown_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ù†Øµ Markdown ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù†Øµ Ø¹Ø§Ø¯ÙŠ"""
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,6}\s+', '', text)
    text = re.sub(r'[_`~\[\]]', '', text)
    text = re.sub(r'^\s*([â€¢\-*+]|\d+\.)\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

# âœ… Ø¯Ø§Ù„Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ· Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ
def register_fonts():
    """ØªØ³Ø¬ÙŠÙ„ Ø®Ø·ÙˆØ· Tahoma Ù…Ù† Ù…Ø¬Ù„Ø¯ fonts"""
    try:
        fonts_dir = os.path.join(os.path.dirname(__file__), "fonts")
        tahoma_path = os.path.join(fonts_dir, "tahoma.ttf")
        tahoma_bold_path = os.path.join(fonts_dir, "tahomabd.ttf")
        
        if os.path.exists(tahoma_path):
            pdfmetrics.registerFont(TTFont("Tahoma", tahoma_path))
        else:
            st.warning(f"âš ï¸ Ù…Ù„Ù Tahoma.ttf ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {fonts_dir}")
            
        if os.path.exists(tahoma_bold_path):
            pdfmetrics.registerFont(TTFont("Tahoma-Bold", tahoma_bold_path))
        else:
            st.warning(f"âš ï¸ Ù…Ù„Ù Tahomabd.ttf ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {fonts_dir}")
            
    except Exception as e:
        st.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·: {e}")

# âœ… Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ù…Ø§Ø· Ù…ÙØ­Ø³Ù‘Ù†Ø©
def create_custom_styles():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ù…Ø§Ø· Ù…Ø®ØµØµØ© Ù„Ù„ØªÙ‚Ø±ÙŠØ±"""
    styles = getSampleStyleSheet()
    
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontName='Tahoma-Bold',
        fontSize=18,
        leading=24,
        alignment=TA_CENTER,
        textColor=HexColor('#1a1a1a'),
        spaceAfter=20,
        spaceBefore=10
    )
    
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Heading2'],
        fontName='Tahoma-Bold',
        fontSize=14,
        leading=20,
        alignment=TA_RIGHT,
        textColor=HexColor('#2c3e50'),
        spaceAfter=12,
        spaceBefore=15
    )
    
    body_style = ParagraphStyle(
        'CustomBody',
        parent=styles['Normal'],
        fontName='Tahoma',
        fontSize=11,
        leading=18,
        alignment=TA_RIGHT,
        textColor=HexColor('#333333'),
        spaceAfter=10,
        spaceBefore=5,
        rightIndent=10,
        leftIndent=10,
        wordWrap='RTL'
    )
    
    summary_style = ParagraphStyle(
        'CustomSummary',
        parent=body_style,
        fontName='Tahoma',
        fontSize=11,
        leading=20,
        backColor=HexColor('#f8f9fa'),
        borderWidth=1,
        borderColor=HexColor('#dee2e6'),
        borderPadding=10,
        borderRadius=3,
        spaceAfter=8,
        spaceBefore=5
    )
    
    defect_title_style = ParagraphStyle(
        'DefectTitle',
        parent=styles['Heading3'],
        fontName='Tahoma-Bold',
        fontSize=12,
        leading=16,
        alignment=TA_RIGHT,
        textColor=HexColor('#e74c3c'),
        spaceAfter=8,
        spaceBefore=12
    )
    
    table_cell_style = ParagraphStyle(
        'TableCell',
        parent=styles['Normal'],
        fontName='Tahoma',
        fontSize=10,
        leading=14,
        alignment=TA_RIGHT,
        textColor=HexColor('#2c3e50'),
        wordWrap='RTL',
        rightIndent=5,
        leftIndent=5
    )
    
    table_header_style = ParagraphStyle(
        'TableHeader',
        parent=table_cell_style,
        fontName='Tahoma-Bold',
        fontSize=10,
        textColor=HexColor('#ffffff'),
        backColor=HexColor('#34495e')
    )
    
    return {
        'title': title_style,
        'subtitle': subtitle_style,
        'body': body_style,
        'summary': summary_style,
        'defect_title': defect_title_style,
        'table_cell': table_cell_style,
        'table_header': table_header_style
    }

# âœ… Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Markdown table Ù„Ù€Table object Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def markdown_to_enhanced_table(md_text: str, styles_dict: dict) -> Table:
    """ØªØ­ÙˆÙŠÙ„ Ø¬Ø¯ÙˆÙ„ Markdown Ø¥Ù„Ù‰ Table object Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø­Ø³Ù‘Ù†"""
    lines = [line.strip() for line in md_text.strip().split('\n') if line.strip()]
    if len(lines) < 2:
        empty_para = Paragraph(process_arabic_text("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"), styles_dict['body'])
        return Table([[empty_para]], colWidths=[6*inch])
    
    header_cells = [cell.strip() for cell in lines[0].split('|') if cell.strip()]
    rows = []
    for line in lines[2:]:
        row_cells = [cell.strip() for cell in line.split('|') if cell.strip()]
        if len(row_cells) == len(header_cells):
            rows.append(row_cells)
    
    num_cols = len(header_cells)
    total_width = 6.5 * inch
    
    col_width_ratios = {
        'Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯': 0.5,
        'Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯': 1.2,
        'Ø§Ù„Ù…ØªØ·Ù„Ø¨': 1.3,
        'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ': 1.2,
        'Ø§Ù„ØªÙˆØµÙŠØ§Øª': 1.5,
        'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­': 1.5,
        'Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)': 0.8
    }
    
    col_widths = []
    for header in header_cells:
        ratio = col_width_ratios.get(header, 1.0)
        col_widths.append(ratio * inch)
    
    processed_data = []
    
    header_row = []
    for cell in header_cells:
        processed_text = process_arabic_text(cell)
        para = Paragraph(processed_text, styles_dict['table_header'])
        header_row.append(para)
    processed_data.append(header_row)
    
    for row in rows:
        row_processed = []
        for col_idx, cell in enumerate(row):
            col_name = header_cells[col_idx]
            
            if col_name in ['Ø§Ù„ØªÙˆØµÙŠØ§Øª', 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­', 'Ø§Ù„Ù…ØªØ·Ù„Ø¨', 'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ']:
                items = [item.strip() for item in re.split(r'[;.]', str(cell)) if item.strip()]
                
                if len(items) > 1:
                    bullet_text = ""
                    for i, item in enumerate(items, 1):
                        if item:
                            bullet_text += f"â€¢ {item}<br/>"
                    
                    processed_text = process_arabic_text(bullet_text)
                    para = Paragraph(processed_text, styles_dict['table_cell'])
                    row_processed.append(para)
                else:
                    processed_text = process_arabic_text(cell)
                    para = Paragraph(processed_text, styles_dict['table_cell'])
                    row_processed.append(para)
            else:
                processed_text = process_arabic_text(cell)
                para = Paragraph(processed_text, styles_dict['table_cell'])
                row_processed.append(para)
        
        processed_data.append(row_processed)
    
    table = Table(processed_data, colWidths=col_widths, repeatRows=1)
    
    table_style = TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor('#34495e')),
        ('TEXTCOLOR', (0, 0), (-1, 0), HexColor('#ffffff')),
        ('FONTNAME', (0, 0), (-1, 0), 'Tahoma-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('ALIGN', (0, 0), (-1, 0), 'RIGHT'),
        ('VALIGN', (0, 0), (-1, 0), 'MIDDLE'),
        
        ('BACKGROUND', (0, 1), (-1, -1), HexColor('#ffffff')),
        ('TEXTCOLOR', (0, 1), (-1, -1), HexColor('#2c3e50')),
        ('FONTNAME', (0, 1), (-1, -1), 'Tahoma'),
        ('FONTSIZE', (0, 1), (-1, -1), 10),
        ('ALIGN', (0, 1), (-1, -1), 'RIGHT'),
        ('VALIGN', (0, 1), (-1, -1), 'TOP'),
        
        ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#bdc3c7')),
        ('LINEBELOW', (0, 0), (-1, 0), 2, HexColor('#2c3e50')),
        
        ('LEFTPADDING', (0, 0), (-1, -1), 8),
        ('RIGHTPADDING', (0, 0), (-1, -1), 8),
        ('TOPPADDING', (0, 0), (-1, -1), 6),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [HexColor('#ffffff'), HexColor('#f8f9fa')]),
    ])
    
    table.setStyle(table_style)
    return table

# âœ… Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ PDF Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def generate_enhanced_pdf_report(images: List[Image.Image], summary: str, tables: List[tuple], defects: List[str]):
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± PDF Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    buffer = io.BytesIO()
    
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ·
    register_fonts()
    
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=50,
        leftMargin=50,
        topMargin=60,
        bottomMargin=40
    )
    
    custom_styles = create_custom_styles()
    
    story = []
    
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    title_text = process_arabic_text("ØªÙ‚Ø±ÙŠØ± ÙØ­Øµ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©")
    story.append(Paragraph(title_text, custom_styles['title']))
    story.append(Spacer(1, 30))
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    date_text = process_arabic_text(f"ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime('%Y-%m-%d')}")
    time_text = process_arabic_text(f"ÙˆÙ‚Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime('%H:%M:%S')}")
    
    info_style = custom_styles['body']
    story.append(Paragraph(date_text, info_style))
    story.append(Paragraph(time_text, info_style))
    story.append(Spacer(1, 20))
    
    # Ø®Ø· ÙØ§ØµÙ„
    from reportlab.platypus import HRFlowable
    story.append(HRFlowable(width="100%", thickness=2, color=HexColor('#3498db'), spaceAfter=20))
    
    # Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…
    summary_title = process_arabic_text("Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…")
    story.append(Paragraph(summary_title, custom_styles['subtitle']))
    story.append(Spacer(1, 10))
    
    cleaned_summary = clean_markdown_text(summary)
    summary_points = [p.strip() for p in cleaned_summary.split('\n') if p.strip()]
    
    for point in summary_points:
        processed_point = process_arabic_text(f"â€¢ {point}")
        story.append(Paragraph(processed_point, custom_styles['summary']))
        story.append(Spacer(1, 5))
    
    story.append(Spacer(1, 20))
    
    # Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±ÙÙ‚Ø©
    images_title = process_arabic_text("Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©")
    story.append(Paragraph(images_title, custom_styles['subtitle']))
    story.append(Spacer(1, 15))
    
    for idx, img in enumerate(images, 1):
        img_resized = img.copy()
        img_resized.thumbnail((350, 350))
        
        img_buffer = io.BytesIO()
        img_resized.save(img_buffer, format='PNG')
        img_buffer.seek(0)
        
        img_caption = process_arabic_text(f"ØµÙˆØ±Ø© Ø±Ù‚Ù… {idx}")
        story.append(Paragraph(img_caption, custom_styles['body']))
        story.append(Spacer(1, 5))
        
        rl_img = RLImage(img_buffer, width=3.5*inch, height=3.5*inch)
        story.append(rl_img)
        story.append(Spacer(1, 15))
    
    story.append(PageBreak())
    
    # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯
    details_title = process_arabic_text("ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯")
    story.append(Paragraph(details_title, custom_styles['subtitle']))
    story.append(Spacer(1, 15))
    
    for defect_name, table_md in tables:
        defect_title = process_arabic_text(f"Ø§Ù„Ø¹ÙŠØ¨: {defect_name}")
        story.append(Paragraph(defect_title, custom_styles['defect_title']))
        story.append(Spacer(1, 10))
        
        table_obj = markdown_to_enhanced_table(table_md, custom_styles)
        story.append(KeepTogether([table_obj]))
        story.append(Spacer(1, 20))
    
    try:
        doc.build(story)
        buffer.seek(0)
        return buffer
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
        import traceback
        st.code(traceback.format_exc())
        return None

# ====== 4. ÙˆØ§Ø¬Ù‡Ø© Streamlit ======
st.set_page_config(page_title="âš¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨", layout="wide")
hide = """<style>#MainMenu{visibility:hidden;}footer{visibility:hidden;}header{visibility:hidden;}</style>"""
st.markdown(hide, unsafe_allow_html=True)

st.markdown("<h1 style='text-align:center;'>âš¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align:center;color:grey;'>Ø­Ù…Ù‘Ù„ÙŠ ØµÙˆØ±Ùƒ ÙˆØ§Ø·Ù‘Ù„Ø¹ÙŠ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ù…ÙØ¬Ù…Ù‘Ø¹ ÙÙŠ Ø«ÙˆØ§Ù†ÙŠ</h4>", unsafe_allow_html=True)

df = load_excel()

if df.empty:
    st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±.")
    st.stop()

docs = df_to_docs(df)
embeddings_matrix, texts = get_vector_db(docs, embedding_model)

uploaded = st.file_uploader("ğŸ“· Ø§Ø±ÙØ¹ÙŠ ØµÙˆØ± Ø§Ù„Ø¹ÙŠÙˆØ¨ (Ù…ØªØ¹Ø¯Ø¯Ø©):", accept_multiple_files=True, type=["jpg", "jpeg", "png"])
if uploaded:
    cols = st.columns(4)
    images = []
    for idx, file in enumerate(uploaded):
        with cols[idx % 4]:
            img = Image.open(file)
            st.image(img, caption=f"ØµÙˆØ±Ø© {idx+1}", use_column_width=True)
            images.append(img)

    if st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„", type="primary", use_container_width=True):
        bar = st.progress(0)
        images_bytes = [f.getvalue() for f in uploaded]
        all_defects = batch_analyze(images_bytes)
        bar.progress(30)

        unique = list(set(all_defects))
        st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ {len(unique)} Ø¹ÙŠØ¨ ÙØ±ÙŠØ¯: {', '.join(unique)}")

        seen = set()
        tables = []
        results = []
        for d in unique:
            # âœ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©
            sim = search_similar(d, docs, embeddings_matrix, texts, embedding_model, k=3)
            band = filter_best_doc(sim, d)
            if band and band not in seen:
                seen.add(band)
                tbl = build_table_from_band(df, band, d)
                tables.append((d, tbl))
                results.append({'query': d, 'doc': sim[0]})
        bar.progress(60)

        combined_queries = '; '.join([r['query'] for r in results])
        
        # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… REST API Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù…Ù„Ø®Øµ
        try:
            import requests
            
            API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
            
            # ØªØ­Ø¶ÙŠØ± Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« (Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙƒØ¨Ø¯ÙŠÙ„)
            context_docs = [r['doc'] for r in results] if results else docs
            context_text = "\n".join([doc.get('content', '') for doc in context_docs[:3]])
            
            qna_prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. Ù‚Ø¯Ù… **Ù…Ù„Ø®Øµ Ø¹Ø§Ù… Ù‚ØµÙŠØ±** Ù„Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ù…Ø¹ **Ø£ÙˆÙ„ÙˆÙŠØ© Ù„ÙƒÙ„ Ø¨Ù†Ø¯** (Ù‚ØµÙˆÙ‰: Ù…Ø®Ø§Ø·Ø± Ø³Ù„Ø§Ù…Ø©ØŒ Ù…ØªÙˆØ³Ø·Ø©: Ø£Ø¯Ø§Ø¡/ØªØ´Ø·ÙŠØ¨ØŒ Ø¹Ø§Ø¯ÙŠØ©: Ø¬Ù…Ø§Ù„ÙŠ). 

**Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ù„Ø®Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø³ØªÙ‚Ù„Ø©ØŒ ÙƒÙ„ Ø¬Ù…Ù„Ø© ÙÙŠ Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯** Ù„ÙˆØµÙ Ø¹ÙŠØ¨ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·ØŒ ÙˆÙ„Ø§ ØªØ¶Ø¹ ØªØ±Ù‚ÙŠÙ… Ø£Ùˆ Ø¨ÙˆÙ„ÙŠØª.

### Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:
{combined_queries}

### Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
{context_text}

### Ø§Ù„Ù…Ù„Ø®Øµ:
"""
            
            payload = {
                "contents": [{
                    "parts": [{"text": qna_prompt}]
                }]
            }
            
            response = requests.post(API_URL, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    summary = result['candidates'][0]['content']['parts'][0]['text']
                else:
                    summary = f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ØªØ§Ù„ÙŠØ©: {combined_queries}"
            else:
                summary = f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ØªØ§Ù„ÙŠØ©: {combined_queries}"
            
        except Exception as e:
            st.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ: {e}")
            summary = f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ØªØ§Ù„ÙŠØ©: {combined_queries}"
        
        bar.progress(90)

        st.subheader("ğŸ“‹ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª")
        st.markdown(summary)

        st.subheader("ğŸ“Š ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯")
        for defect, tbl in tables:
            with st.expander(f"ğŸ” {defect}"):
                st.markdown(tbl)

        # âœ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
        pdf_buffer = generate_enhanced_pdf_report(images, summary, tables, unique)
        bar.progress(100)
        
        if pdf_buffer:
            st.success("âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­!")
            st.download_button(
                label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (PDF)",
                data=pdf_buffer,
                file_name=f"ØªÙ‚Ø±ÙŠØ±_Ø§Ù„Ø¹ÙŠÙˆØ¨_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                mime="application/pdf",
                type="primary",
                use_container_width=True
            )
        else:
            st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

# ====== 5. Footer ======
st.markdown("---")
st.markdown("<p style='text-align:center;color:grey;'>âš¡ Ù†Ø¸Ø§Ù… Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© | ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© AI</p>", unsafe_allow_html=True)