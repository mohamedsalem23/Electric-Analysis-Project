# ====== 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø³Ø±ÙŠØ¹Ø© ======
import os, streamlit as st, pandas as pd, re, io, base64
from typing import List
from PIL import Image
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage
from langchain import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# âœ… Ø¥Ø¶Ø§ÙØ§Øª PDF
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak, KeepTogether
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.colors import HexColor, black, grey
from datetime import datetime

# âœ… Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„
from reportlab.lib.enums import TA_RIGHT, TA_CENTER
from bidi import algorithm as bidi_algorithm
from arabic_reshaper import reshape 

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# âœ… Ù‚Ø±Ø§Ø¡Ø© API Key Ù…Ù† Streamlit Secrets
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    if not GEMINI_API_KEY:
        st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© GEMINI_API_KEY ÙÙŠ Settings â†’ Secrets Ø¹Ù„Ù‰ Streamlit Cloud")
        st.info("Ù„Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø­Ù„ÙŠ: Ø£Ù†Ø´Ø¦ Ù…Ù„Ù .streamlit/secrets.toml ÙˆØ¶Ø¹ ÙÙŠÙ‡: GEMINI_API_KEY = 'your_key'")
        st.stop()

# ====== 2. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ======
def pil_to_base64_uri(image: Image.Image, fmt="PNG") -> str:
    buf = io.BytesIO()
    image.save(buf, format=fmt)
    img_bytes = buf.getvalue()
    return f"data:image/{fmt.lower()};base64,{base64.b64encode(img_bytes).decode()}"

@st.cache_data(show_spinner=False)
def load_excel() -> pd.DataFrame:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Excel Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ"""
    try:
        # âœ… Ù…Ø³Ø§Ø± Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù€ Deploy
        excel_path = os.path.join(os.path.dirname(__file__), "data", "Ø¬Ù…ÙŠØ¹_Ø¨Ù†ÙˆØ¯_ÙØ­Øµ_Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.xlsx")
        
        if not os.path.exists(excel_path):
            st.error(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {excel_path}")
            st.info("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ data/")
            # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            return pd.DataFrame({
                "Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯": [5]*4,
                "Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯": ["Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©"]*4,
                "Ø§Ù„Ù…ØªØ·Ù„Ø¨": ["..."],
                "Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ": ["..."],
                "Ø§Ù„ØªÙˆØµÙŠØ§Øª": ["ÙŠØ¬Ø¨ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙØªØ§Ø­ Ø¬ÙŠØ¯Ø§Ù‹.; ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø­ÙˆÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±."],
                "Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­": ["Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠÙ„ÙŠÙƒÙˆÙ† Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§ØºØ§Øª.; Ø¥Ø¹Ø§Ø¯Ø© ØªØ«Ø¨ÙŠØª Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚ÙŠÙ…."],
                "Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)": [35,30,40,25]
            })
        
        return pd.read_excel(excel_path)
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
        return pd.DataFrame()

@st.cache_data(show_spinner=False)
def df_to_docs(df: pd.DataFrame) -> List[Document]:
    return [Document(page_content=f"Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯: {r['Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯']}. Ø§Ù„Ù…ØªØ·Ù„Ø¨: {r['Ø§Ù„Ù…ØªØ·Ù„Ø¨']}.", metadata=r.to_dict())
            for _, r in df.iterrows()]

def filter_best_doc(similar_docs: List[Document], query: str) -> int:
    best_doc = None
    best_score = 0.0
    for doc in similar_docs:
        name = doc.metadata.get('Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯', '')
        match_score = len(set(re.findall(r'\w+', query.lower())) & set(re.findall(r'\w+', name.lower()))) / max(len(set(re.findall(r'\w+', query.lower()))), 1)
        if match_score > best_score:
            best_score = match_score
            best_doc = doc
    return int(best_doc.metadata.get('Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 0)) if best_doc else int(similar_docs[0].metadata.get('Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 0))

def build_table_from_band(dataframe: pd.DataFrame, band_num: int, query: str) -> str:
    band_rows = dataframe[dataframe['Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯'] == band_num].copy()
    if band_rows.empty:
        return "| Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª |"
    def match_score(row):
        req = str(row.get('Ø§Ù„Ù…ØªØ·Ù„Ø¨', '')).lower()
        q_words = set(re.findall(r'\w+', query.lower()))
        row_words = set(re.findall(r'\w+', req))
        return len(q_words & row_words) / max(len(q_words), 1)
    band_rows['match_score'] = band_rows.apply(match_score, axis=1)
    best_row = band_rows.loc[band_rows['match_score'].idxmax()]
    cols = ['Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯', 'Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯', 'Ø§Ù„Ù…ØªØ·Ù„Ø¨', 'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ', 'Ø§Ù„ØªÙˆØµÙŠØ§Øª', 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­', 'Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)']
    best_row = best_row[cols].to_frame().T
    return best_row.to_markdown(index=False)

@st.cache_resource(show_spinner=False)
def get_models():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    chat = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model="gemini-2.5-flash", temperature=0)
    return embeddings, chat

embeddings, chat = get_models()

@st.cache_resource(show_spinner=False)
def get_vector_db(_docs: List[Document]):
    persist_dir = "chroma_db"
    if os.path.isdir(persist_dir):
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    db = Chroma.from_documents(_docs, embeddings, persist_directory=persist_dir)
    return db

@st.cache_data(show_spinner=False)
def batch_analyze(images_bytes: List[bytes]) -> List[str]:
    prompt = """
    Ø£Ù†Øª Ù†Ø¸Ø§Ù… Ø±Ø¤ÙŠØ© Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…ØªØ®ØµØµ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØªØ­Ø¯ÙŠØ¯ **Ø¬Ù…ÙŠØ¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©** Ø§Ù„Ù„ÙŠ ØªØ¸Ù‡Ø± (Ø­ØªÙ‰ Ù„Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† ÙˆØ§Ø­Ø¯Ø©ØŒ Ù…Ø«Ù„ ÙØ±Ø§ØºØ§Øª + Ù…ÙŠÙ„Ø§Ù† + Ø¨Ø±ÙˆØ²). 
    **Ù„ÙƒÙ„ Ø¹ÙŠØ¨ØŒ Ø£Ø¹Ø·Ù Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ (Ø£Ùˆ Ø§Ù„Ø£Ù‚Ø±Ø¨) Ù…Ù† Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©**ØŒ ÙˆÙØµÙ„Ù‡Ø§ Ø¨Ù€ ';' (Ù…Ø«Ù„: 'Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ´Ø·ÙŠØ¨ Ø­ÙˆÙ„ Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©; Ø§Ø³ØªÙ‚Ø§Ù…Ø© Ø§Ù„Ø£ÙÙŠØ§Ø´ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ø£ÙÙ‚ÙŠÙ‹Ø§').
    Ù„Ùˆ Ø¹ÙŠØ¨ ÙˆØ§Ø­Ø¯ØŒ Ø£Ø¹Ø·Ù Ø§Ø³Ù…Ù‡ Ø¨Ø³. Ù„Ø§ ØªØ¶Ù ØªÙØ³ÙŠØ± Ø£Ùˆ Ø´Ø±Ø­ØŒ Ù†Ø§ØªØ¬Ùƒ Ù†Øµ ÙˆØ§Ø­Ø¯ Ù…ÙØµÙˆÙ„ Ø¨Ù€ ';'.
    """
    content = [{"type": "text", "text": prompt}]
    for img_bytes in images_bytes:
        img = Image.open(io.BytesIO(img_bytes))
        uri = pil_to_base64_uri(img)
        content.append({"type": "image_url", "image_url": {"url": uri}})
    msg = HumanMessage(content=content)
    resp = chat.invoke([msg])
    lines = resp.content.strip().splitlines()
    defects = []
    for line in lines:
        defects.extend([x.strip() for x in line.split(";") if x.strip()])
    return defects

# âœ… Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def process_arabic_text(text: str) -> str:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ø¹ Ø¯Ø¹Ù… RTL"""
    if not text or text == "nan" or pd.isna(text):
        return "â€”"
    text = str(text).strip()
    reshaped = reshape(text)
    bidi_text = bidi_algorithm.get_display(reshaped)
    return bidi_text

# âœ… Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Markdown Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def clean_markdown_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ù†Øµ Markdown ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù†Øµ Ø¹Ø§Ø¯ÙŠ"""
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,6}\s+', '', text)
    text = re.sub(r'[_`~\[\]]', '', text)
    text = re.sub(r'^\s*([â€¢\-*+]|\d+\.)\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

# âœ… Ø¯Ø§Ù„Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ· Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ
def register_fonts():
    """ØªØ³Ø¬ÙŠÙ„ Ø®Ø·ÙˆØ· Tahoma Ù…Ù† Ù…Ø¬Ù„Ø¯ fonts"""
    try:
        fonts_dir = os.path.join(os.path.dirname(__file__), "fonts")
        tahoma_path = os.path.join(fonts_dir, "Tahoma.ttf")
        tahoma_bold_path = os.path.join(fonts_dir, "Tahomabd.ttf")
        
        if os.path.exists(tahoma_path):
            pdfmetrics.registerFont(TTFont("Tahoma", tahoma_path))
        else:
            st.warning(f"âš ï¸ Ù…Ù„Ù Tahoma.ttf ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {fonts_dir}")
            
        if os.path.exists(tahoma_bold_path):
            pdfmetrics.registerFont(TTFont("Tahoma-Bold", tahoma_bold_path))
        else:
            st.warning(f"âš ï¸ Ù…Ù„Ù Tahomabd.ttf ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {fonts_dir}")
            
    except Exception as e:
        st.warning(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·: {e}")

# âœ… Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ù…Ø§Ø· Ù…ÙØ­Ø³Ù‘Ù†Ø©
def create_custom_styles():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ù…Ø§Ø· Ù…Ø®ØµØµØ© Ù„Ù„ØªÙ‚Ø±ÙŠØ±"""
    styles = getSampleStyleSheet()
    
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontName='Tahoma-Bold',
        fontSize=18,
        leading=24,
        alignment=TA_CENTER,
        textColor=HexColor('#1a1a1a'),
        spaceAfter=20,
        spaceBefore=10
    )
    
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Heading2'],
        fontName='Tahoma-Bold',
        fontSize=14,
        leading=20,
        alignment=TA_RIGHT,
        textColor=HexColor('#2c3e50'),
        spaceAfter=12,
        spaceBefore=15
    )
    
    body_style = ParagraphStyle(
        'CustomBody',
        parent=styles['Normal'],
        fontName='Tahoma',
        fontSize=11,
        leading=18,
        alignment=TA_RIGHT,
        textColor=HexColor('#333333'),
        spaceAfter=10,
        spaceBefore=5,
        rightIndent=10,
        leftIndent=10,
        wordWrap='RTL'
    )
    
    summary_style = ParagraphStyle(
        'CustomSummary',
        parent=body_style,
        fontName='Tahoma',
        fontSize=11,
        leading=20,
        backColor=HexColor('#f8f9fa'),
        borderWidth=1,
        borderColor=HexColor('#dee2e6'),
        borderPadding=10,
        borderRadius=3,
        spaceAfter=8,
        spaceBefore=5
    )
    
    defect_title_style = ParagraphStyle(
        'DefectTitle',
        parent=styles['Heading3'],
        fontName='Tahoma-Bold',
        fontSize=12,
        leading=16,
        alignment=TA_RIGHT,
        textColor=HexColor('#e74c3c'),
        spaceAfter=8,
        spaceBefore=12
    )
    
    table_cell_style = ParagraphStyle(
        'TableCell',
        parent=styles['Normal'],
        fontName='Tahoma',
        fontSize=10,
        leading=14,
        alignment=TA_RIGHT,
        textColor=HexColor('#2c3e50'),
        wordWrap='RTL',
        rightIndent=5,
        leftIndent=5
    )
    
    table_header_style = ParagraphStyle(
        'TableHeader',
        parent=table_cell_style,
        fontName='Tahoma-Bold',
        fontSize=10,
        textColor=HexColor('#ffffff'),
        backColor=HexColor('#34495e')
    )
    
    return {
        'title': title_style,
        'subtitle': subtitle_style,
        'body': body_style,
        'summary': summary_style,
        'defect_title': defect_title_style,
        'table_cell': table_cell_style,
        'table_header': table_header_style
    }

# âœ… Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Markdown table Ù„Ù€Table object Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def markdown_to_enhanced_table(md_text: str, styles_dict: dict) -> Table:
    """ØªØ­ÙˆÙŠÙ„ Ø¬Ø¯ÙˆÙ„ Markdown Ø¥Ù„Ù‰ Table object Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø­Ø³Ù‘Ù†"""
    lines = [line.strip() for line in md_text.strip().split('\n') if line.strip()]
    if len(lines) < 2:
        empty_para = Paragraph(process_arabic_text("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"), styles_dict['body'])
        return Table([[empty_para]], colWidths=[6*inch])
    
    header_cells = [cell.strip() for cell in lines[0].split('|') if cell.strip()]
    rows = []
    for line in lines[2:]:
        row_cells = [cell.strip() for cell in line.split('|') if cell.strip()]
        if len(row_cells) == len(header_cells):
            rows.append(row_cells)
    
    num_cols = len(header_cells)
    total_width = 6.5 * inch
    
    col_width_ratios = {
        'Ø±Ù‚Ù… Ø§Ù„Ø¨Ù†Ø¯': 0.5,
        'Ø§Ø³Ù… Ø§Ù„Ø¨Ù†Ø¯': 1.2,
        'Ø§Ù„Ù…ØªØ·Ù„Ø¨': 1.3,
        'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ': 1.2,
        'Ø§Ù„ØªÙˆØµÙŠØ§Øª': 1.5,
        'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­': 1.5,
        'Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠØ±ÙŠØ© (Ø±ÙŠØ§Ù„)': 0.8
    }
    
    col_widths = []
    for header in header_cells:
        ratio = col_width_ratios.get(header, 1.0)
        col_widths.append(ratio * inch)
    
    processed_data = []
    
    header_row = []
    for cell in header_cells:
        processed_text = process_arabic_text(cell)
        para = Paragraph(processed_text, styles_dict['table_header'])
        header_row.append(para)
    processed_data.append(header_row)
    
    for row in rows:
        row_processed = []
        for col_idx, cell in enumerate(row):
            col_name = header_cells[col_idx]
            
            if col_name in ['Ø§Ù„ØªÙˆØµÙŠØ§Øª', 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­', 'Ø§Ù„Ù…ØªØ·Ù„Ø¨', 'Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ']:
                items = [item.strip() for item in re.split(r'[;.]', str(cell)) if item.strip()]
                
                if len(items) > 1:
                    bullet_text = ""
                    for i, item in enumerate(items, 1):
                        if item:
                            bullet_text += f"â€¢ {item}<br/>"
                    
                    processed_text = process_arabic_text(bullet_text)
                    para = Paragraph(processed_text, styles_dict['table_cell'])
                    row_processed.append(para)
                else:
                    processed_text = process_arabic_text(cell)
                    para = Paragraph(processed_text, styles_dict['table_cell'])
                    row_processed.append(para)
            else:
                processed_text = process_arabic_text(cell)
                para = Paragraph(processed_text, styles_dict['table_cell'])
                row_processed.append(para)
        
        processed_data.append(row_processed)
    
    table = Table(processed_data, colWidths=col_widths, repeatRows=1)
    
    table_style = TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor('#34495e')),
        ('TEXTCOLOR', (0, 0), (-1, 0), HexColor('#ffffff')),
        ('FONTNAME', (0, 0), (-1, 0), 'Tahoma-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('ALIGN', (0, 0), (-1, 0), 'RIGHT'),
        ('VALIGN', (0, 0), (-1, 0), 'MIDDLE'),
        
        ('BACKGROUND', (0, 1), (-1, -1), HexColor('#ffffff')),
        ('TEXTCOLOR', (0, 1), (-1, -1), HexColor('#2c3e50')),
        ('FONTNAME', (0, 1), (-1, -1), 'Tahoma'),
        ('FONTSIZE', (0, 1), (-1, -1), 10),
        ('ALIGN', (0, 1), (-1, -1), 'RIGHT'),
        ('VALIGN', (0, 1), (-1, -1), 'TOP'),
        
        ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#bdc3c7')),
        ('LINEBELOW', (0, 0), (-1, 0), 2, HexColor('#2c3e50')),
        
        ('LEFTPADDING', (0, 0), (-1, -1), 8),
        ('RIGHTPADDING', (0, 0), (-1, -1), 8),
        ('TOPPADDING', (0, 0), (-1, -1), 6),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [HexColor('#ffffff'), HexColor('#f8f9fa')]),
    ])
    
    table.setStyle(table_style)
    return table

# âœ… Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ PDF Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
def generate_enhanced_pdf_report(images: List[Image.Image], summary: str, tables: List[tuple], defects: List[str]):
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± PDF Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    buffer = io.BytesIO()
    
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ·
    register_fonts()
    
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=50,
        leftMargin=50,
        topMargin=60,
        bottomMargin=40
    )
    
    custom_styles = create_custom_styles()
    
    story = []
    
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    title_text = process_arabic_text("ØªÙ‚Ø±ÙŠØ± ÙØ­Øµ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©")
    story.append(Paragraph(title_text, custom_styles['title']))
    story.append(Spacer(1, 30))
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    date_text = process_arabic_text(f"ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime('%Y-%m-%d')}")
    time_text = process_arabic_text(f"ÙˆÙ‚Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime('%H:%M:%S')}")
    
    info_style = custom_styles['body']
    story.append(Paragraph(date_text, info_style))
    story.append(Paragraph(time_text, info_style))
    story.append(Spacer(1, 20))
    
    # Ø®Ø· ÙØ§ØµÙ„
    from reportlab.platypus import HRFlowable
    story.append(HRFlowable(width="100%", thickness=2, color=HexColor('#3498db'), spaceAfter=20))
    
    # Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…
    summary_title = process_arabic_text("Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…")
    story.append(Paragraph(summary_title, custom_styles['subtitle']))
    story.append(Spacer(1, 10))
    
    cleaned_summary = clean_markdown_text(summary)
    summary_points = [p.strip() for p in cleaned_summary.split('\n') if p.strip()]
    
    for point in summary_points:
        processed_point = process_arabic_text(f"â€¢ {point}")
        story.append(Paragraph(processed_point, custom_styles['summary']))
        story.append(Spacer(1, 5))
    
    story.append(Spacer(1, 20))
    
    # Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±ÙÙ‚Ø©
    images_title = process_arabic_text("Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©")
    story.append(Paragraph(images_title, custom_styles['subtitle']))
    story.append(Spacer(1, 15))
    
    for idx, img in enumerate(images, 1):
        img_resized = img.copy()
        img_resized.thumbnail((350, 350))
        
        img_buffer = io.BytesIO()
        img_resized.save(img_buffer, format='PNG')
        img_buffer.seek(0)
        
        img_caption = process_arabic_text(f"ØµÙˆØ±Ø© Ø±Ù‚Ù… {idx}")
        story.append(Paragraph(img_caption, custom_styles['body']))
        story.append(Spacer(1, 5))
        
        rl_img = RLImage(img_buffer, width=3.5*inch, height=3.5*inch)
        story.append(rl_img)
        story.append(Spacer(1, 15))
    
    story.append(PageBreak())
    
    # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯
    details_title = process_arabic_text("ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯")
    story.append(Paragraph(details_title, custom_styles['subtitle']))
    story.append(Spacer(1, 15))
    
    for defect_name, table_md in tables:
        defect_title = process_arabic_text(f"Ø§Ù„Ø¹ÙŠØ¨: {defect_name}")
        story.append(Paragraph(defect_title, custom_styles['defect_title']))
        story.append(Spacer(1, 10))
        
        table_obj = markdown_to_enhanced_table(table_md, custom_styles)
        story.append(KeepTogether([table_obj]))
        story.append(Spacer(1, 20))
    
    try:
        doc.build(story)
        buffer.seek(0)
        return buffer
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
        import traceback
        st.code(traceback.format_exc())
        return None

# ====== 4. ÙˆØ§Ø¬Ù‡Ø© Streamlit ======
st.set_page_config(page_title="âš¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨", layout="wide")
hide = """<style>#MainMenu{visibility:hidden;}footer{visibility:hidden;}header{visibility:hidden;}</style>"""
st.markdown(hide, unsafe_allow_html=True)

st.markdown("<h1 style='text-align:center;'>âš¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align:center;color:grey;'>Ø­Ù…Ù‘Ù„ÙŠ ØµÙˆØ±Ùƒ ÙˆØ§Ø·Ù‘Ù„Ø¹ÙŠ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ù…ÙØ¬Ù…Ù‘Ø¹ ÙÙŠ Ø«ÙˆØ§Ù†ÙŠ</h4>", unsafe_allow_html=True)

df = load_excel()

if df.empty:
    st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±.")
    st.stop()

docs = df_to_docs(df)
vector_db = get_vector_db(docs)

uploaded = st.file_uploader("ğŸ“· Ø§Ø±ÙØ¹ÙŠ ØµÙˆØ± Ø§Ù„Ø¹ÙŠÙˆØ¨ (Ù…ØªØ¹Ø¯Ø¯Ø©):", accept_multiple_files=True, type=["jpg", "jpeg", "png"])
if uploaded:
    cols = st.columns(4)
    images = []
    for idx, file in enumerate(uploaded):
        with cols[idx % 4]:
            img = Image.open(file)
            st.image(img, caption=f"ØµÙˆØ±Ø© {idx+1}", use_column_width=True)
            images.append(img)

    if st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„", type="primary", use_container_width=True):
        bar = st.progress(0)
        images_bytes = [f.getvalue() for f in uploaded]
        all_defects = batch_analyze(images_bytes)
        bar.progress(30)

        unique = list(set(all_defects))
        st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ {len(unique)} Ø¹ÙŠØ¨ ÙØ±ÙŠØ¯: {', '.join(unique)}")

        seen = set()
        tables = []
        results = []
        for d in unique:
            sim = vector_db.similarity_search(d, k=3)
            band = filter_best_doc(sim, d)
            if band and band not in seen:
                seen.add(band)
                tbl = build_table_from_band(df, band, d)
                tables.append((d, tbl))
                results.append({'query': d, 'doc': sim[0]})
        bar.progress(60)

        combined_queries = '; '.join([r['query'] for r in results])
        qna_template = """
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ©. Ù‚Ø¯Ù… **Ù…Ù„Ø®Øµ Ø¹Ø§Ù… Ù‚ØµÙŠØ±** Ù„Ù„Ø¹ÙŠÙˆØ¨ØŒ Ù…Ø¹ **Ø£ÙˆÙ„ÙˆÙŠØ© Ù„ÙƒÙ„ Ø¨Ù†Ø¯** (Ù‚ØµÙˆÙ‰: Ù…Ø®Ø§Ø·Ø± Ø³Ù„Ø§Ù…Ø©ØŒ Ù…ØªÙˆØ³Ø·Ø©: Ø£Ø¯Ø§Ø¡/ØªØ´Ø·ÙŠØ¨ØŒ Ø¹Ø§Ø¯ÙŠØ©: Ø¬Ù…Ø§Ù„ÙŠ). **Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ù„Ø®Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø³ØªÙ‚Ù„Ø©ØŒ ÙƒÙ„ Ø¬Ù…Ù„Ø© ÙÙŠ Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯** Ù„ÙˆØµÙ Ø¹ÙŠØ¨ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·ØŒ ÙˆÙ„Ø§ ØªØ¶Ø¹ ØªØ±Ù‚ÙŠÙ… Ø£Ùˆ Ø¨ÙˆÙ„ÙŠØª.

### Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

### Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}

### Ø§Ù„Ù…Ù„Ø®Øµ:
"""
        qna_prompt = PromptTemplate(template=qna_template, input_variables=["context", "question"])
        stuff_chain = load_qa_chain(chat, chain_type="stuff", prompt=qna_prompt)

        context_docs = [r['doc'] for r in results]
        answer = stuff_chain({"input_documents": context_docs, "question": combined_queries}, return_only_outputs=True)
        summary = answer["output_text"]
        bar.progress(90)

        st.subheader("ğŸ“‹ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù… ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª")
        st.markdown(summary)

        st.subheader("ğŸ“Š ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ù†ÙˆØ¯")
        for defect, tbl in tables:
            with st.expander(f"ğŸ” {defect}"):
                st.markdown(tbl)

        # âœ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
        pdf_buffer = generate_enhanced_pdf_report(images, summary, tables, unique)
        bar.progress(100)
        
        if pdf_buffer:
            st.success("âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­!")
            st.download_button(
                label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (PDF)",
                data=pdf_buffer,
                file_name=f"ØªÙ‚Ø±ÙŠØ±_Ø§Ù„Ø¹ÙŠÙˆØ¨_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                mime="application/pdf",
                type="primary",
                use_container_width=True
            )
        else:
            st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

# ====== 5. Footer ======
st.markdown("---")
st.markdown("<p style='text-align:center;color:grey;'>âš¡ Ù†Ø¸Ø§Ù… Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© | ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© AI</p>", unsafe_allow_html=True)